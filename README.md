## Big Data with examples and Types:
 Big data means collection of large data which cannot be processed or analysed by traditional data base management systems.
 " It is different from an ordinary data because of
 its high volume, high velocity, and heterogeneous variety."[1]

 ### Examples:
 1. Social media like twiter, instagram, facebook generated huge data in the form of text, images and videos.
 2. Demographic data
 3. Transactional data
 4. Customer created feedback/ reviews texts.
 5. Web data
 6. Healthcare data
 7. Sensor data - as everything is becoming more digitalized(" Smart"), huge amount of data is being generated in indutries and day to day lives.

 ### Types:
 1. Batch data - This is a type of data which is collected over time.
 2. Stream data - This is live or real time data
 3. Spatio temporal - real time, time varying data generated by satellities, GPS data and wireless communication devices.
 "Recent rapid development of wireless communication, mobile computing, global navigational satellite systems (GNSS), and spatially enabled sensors is leading to an exponential growth of available spatio-temporal data produced continuously at high speed."[2]

## 6 ‘V’s of Big Data (define each) 
#### Volume:
Big data itself means enormous amount of data, which cant be handled by traditional systems and requires specialized tools and techniques to process the data. "With regards to Volume, it refers to that the data scale is very large, ranging from several PB (1000TB) to  ZB(a billion TB)"[3]
#### Velocity:
This basically refers to how fast the data is being generated or how fast the data is being processed. For example, the data generated online( through websites/apps) is high speed data.
#### Variety:
With increase in the sources of data, various forms of data is being generated like text, images, videos, graphs, audio. These can be classified as structured(tabular data,relational data), unstructured(text, images, audio, video,logs) and semi-structured data (XML,JSON data, graphs).
#### Value:
This refers to meaningful or usefulness property of data. The data can provide more insights if it has more value. "The signicance of big data is not the  great volume, but rather the huge value. How to extract the  value from massive data through powerful algorithms, is the key to improve competitiveness".[3]
#### Variability: 
This is simply, the inconsistencies in data or how spread out the data is. The data can also be said it is variable if there are so many different types and sources of data.
#### Veracity: 
This refers to the accuracy of data / Trust in data that is being used for analysis. If the data is inaccurate/ incomplete then it will be difficult to obtain insights from it.
                                   
## Phases of Big Data analysis (discuss each)
### Data Acquistion: 
As per the business requirements,data can be collected from various sources(sensors, websites, social media, databases). These sources may generate data in a variety of formats, such as structured (relational databases, Excel files), semi-structured (JSON, XML), and unstructured (text, photos, and videos). The primary challenge in data acquisition is the need to filter the collected data, which involves the removal of unwanted information. Unwanted data can take the form of duplicates, irrelevant information, or noise. It is of utmost importance to use the appropriate set of filters, as incorrect filtering could result in the loss of valuable data.
### Data Cleaning:
The first step is to convert the filtered data into a structured format. Afterward, the process of data cleaning involves keeping the essential information while removing incorrect or noisy data."Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them."[4] 
### Data Aggregation & Representation:
"Data aggregation refers to the process by which raw data are gathered, reformatted, and presented in a summary form for subsequent data sharing and further analyses."[5]
In today's world, data aggregation poses a challenge due to the diversity of data sources.For example, if one source may have collected data over time and another one source may have actively generating data. In these scenarios, time definitely cannot be the aggregative function and to have some common factor to merge/aggregate these datas will be more difficult.
Data Representation refers to the different ways in which the same data can be visually presented. This includes various visualization techniques such as bar charts, histograms, box plots, and pie charts.
### Data Analysis:
The steps involved in data analysis for big data differs when compared to traditional data sets. In the smaller data sets by using querying for example, using SQL we can obtain information. But, this is not in the case of big data as the data is more complex."Big Data is noisy, dynamic, heterogeneous, inter-related and untrustworthy. But even noisy Big Data can be valuable, as reliable hidden patterns and knowledge can be disclosed. It also avoid from overpower individual fluctuations and other small sampling biases."[6]
### Interpretation:
This phase is where we understand what the results of our analysis are. Visualizing the data can be a big help in making it easier to understand and share but, it's more than just showing the data through visualisations. When interpreting data, we don't just describe what the data shows, we should also think about what it might mean for the business.With the information like where the data came from, how we got each result, and what specific information we used, we can more valuable insights from the data. 

## Challenges in Big Data analysis (discuss each)
### Heterogeneity and Incompleteness:
"Heterogeneous data are any data with high variability of data types and formats. They are possibly ambiguous and low quality due to missing values, high data redundancy, and untruthfulness"[7]. This is huge challenge to aggregate this data to perform further analysis as the machine learning algorithms will give accurate results with homogeneous aggregated data sets. It requires a lot of skill and knowledge from the experts to deal with this type of data and to get accurate insights.
### Scale:
Big data projects face numerous challenges tied to scalability, particularly as experts anticipate exponential data growth. Subsequently, there arises the question of where to store this large data sets. With the increased likelihood of system failures due to the complexities of handling such vast datasets, cloud computing and parallel file systems present flexible solutions. However, organizations must carefully plan their architecture, storage decisions, and parallel computing to optimize cost-effectiveness according to the project's specific requirements.
### Timeliness:
In today's world, timeliness has become a crucial factor and a significant challenge due to the prevalence of real-time data from numerous sources. For instance, consider the vast and constantly updating stock market data, where there is a demanding need for instantaneous analysis at every moment. Dealing with this continuous stream of data presents a challenge, especially when trying to single out a specific dataset from the vast pool of big data, and performing accurate cleaning within a short timeframe. Even if we manage to obtain an analysis report, it can become obsolete rapidly if the data changes swiftly. "In a real-time system, the correctness of a computation depends on both the logical correctness of the results, and the time at which the computation completes. The property of completing the computation by a given deadline is referred to as timeliness"[8]. 
### Privacy: 
This is the biggest concern that needs to be taken care of by the organisations when collecting data. Because of the IoT everything is connected and there is alot of data being gathered. For example, in the case of smart cities the user geographical or location data is being collected by the sensor devices. This is a privacy concern for the individuals as their home or work addresses will be collected by the organisations, and it is even more a big problem if the locations of government or security agencies are collected. In the era of data leaks and compromises having this type of sensitive information is a huge risk. Secondly, there is another ethical challenge that the organisation might misuse the data. It is the responsibility of both the users and companies to face this challenge.
### Human Collaboration:
In the analysis of big data, human intervention is necessary at every stage. Consider, for example, the data cleaning phase, which is crucial due to the inherent heterogeneity and dynamic nature of source data. During this process, it is imperative to ensure that valuable information is not getting filtered out along with the noise. Throughout the entire data analysis phases, maintaining accuracy, timeliness, security, and resilience in the face of potential system failures remains paramount. This necessitates continuous collaboration with experts at every juncture.

## References:
1. M. M. Rathore, S. A. Shah, D. Shukla, E. Bentafat and S. Bakiras, "The Role of AI, 
   Machine Learning, and Big Data in Digital Twinning: A Systematic Literature Review, 
   Challenges, and Opportunities," in IEEE Access, vol. 9, pp.32030-32052, 2021, 
   doi:10.1109/ACCESS.2021.3060863.
2. Galić, Z. (2016). Spatio-Temporal Data Streams and Big Data Paradigm. In: Spatio-Temporal Data Streams.
   SpringerBriefs in Computer Science. Springer, New York, NY. https://doi.org/10.1007/978-1-4939-6575-5_3
3. Q. Qi and F. Tao, "Digital Twin and Big Data Towards Smart Manufacturing and Industry 
   4.0: 360 Degree Comparison," in IEEE Access, vol. 6, pp. 3585-3593, 2018,
   doi: 10.1109/ACCESS.2018.2793265.
4. Ridzuan, F., & Zainon, W. M. N. W. (2019). A review on data cleansing methods for big data. Procedia Computer Science, 161, 731–738. 
   https://doi.org/10.1016/j.procs.2019.11.177
5. Wen, T. (2020). Data Aggregation. In: Schintler, L., McNeely, C. (eds) Encyclopedia of Big Data. Springer, Cham. 
   https://doi.org/10.1007/978-3-319-32001-4_296-1
6. Zhao, C. (n.d.). Five Phases in the Big Data Processing Pipeline (notes). www.linkedin.com. https://www.linkedin.com/pulse/five- 
   phases-big-data-processing-pipeline-notes-christiane-zhao/
7. Wang, L. (2017). Heterogeneous data and big data analytics. Automatic Control and Information Sciences, 3(1), 8–15. 
   https://doi.org/10.12691/acis-3-1-3
8. Cai, S., Gallina, B., Nyström, D., & Seceleanu, C. (2018). Data aggregation processes: a survey, a taxonomy, and design guidelines. 
   Computing, 101(10), 1397–1429. https://doi.org/10.1007/s00607-018-0679-5
9. Indicative Analytics. (2021, September 15). What is data variability? Data defined - indicative. Indicative. 
   https://www.indicative.com/resource/data-variability/
10. GeeksforGeeks. (2021). Big Data Analytics life cycle. GeeksforGeeks. https://www.geeksforgeeks.org/big-data-analytics-life-cycle/

 
